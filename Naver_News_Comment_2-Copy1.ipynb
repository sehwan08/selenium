{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a518bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException, ElementNotInteractableException  #(클릭시 없을때, 엘리멘트 자체가 없을떄, 엘리멘트가 상호작용을 못할때 )\n",
    "\n",
    "\n",
    "#웹브라우저를 띄우지 않고 진행하기 위한 설정\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5dd5de",
   "metadata": {},
   "source": [
    "### news_scraping() : 뉴스 스크래핑 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a661baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_scraping(news_url, wd):\n",
    "    \n",
    "    press = wd.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[1]/div[1]/a/img').get_attribute('title')\n",
    "    title = wd.find_element(By.ID, 'articleTitle').text\n",
    "    datetime = wd.find_element(By.CLASS_NAME, 't11').text\n",
    "    article = wd.find_element(By.ID, 'articleBodyContents').text\n",
    "    article = article.replace(\"// flash 오류를 우회하기 위한 함수 추가\", \"\")\n",
    "    article = article.replace(\"function _flash_removeCallback() {}\", \"\")\n",
    "    article = article.replace(\"\\n\", \"\")\n",
    "    article = article.replace(\"\\t\", \"\")\n",
    "    \n",
    "    good = wd.find_element(By.XPATH, '//*[@id=\"spiLayer\"]/div[1]/ul/li[1]/a/span[2]').text\n",
    "    warm = wd.find_element(By.XPATH, '//*[@id=\"spiLayer\"]/div[1]/ul/li[2]/a/span[2]').text\n",
    "    sad = wd.find_element(By.XPATH, '//*[@id=\"spiLayer\"]/div[1]/ul/li[3]/a/span[2]').text\n",
    "    angry = wd.find_element(By.XPATH, '//*[@id=\"spiLayer\"]/div[1]/ul/li[4]/a/span[2]').text\n",
    "    want = wd.find_element(By.XPATH, '//*[@id=\"spiLayer\"]/div[1]/ul/li[5]/a/span[2]').text\n",
    "    recommend = wd.find_element(By.XPATH, '//*[@id=\"toMainContainer\"]/a/em[2]').text\n",
    "    \n",
    "#     print(\"뉴스: \", [title, press, datetime, article, good, warm, sad, angry, want, recommend, news_url])\n",
    "    \n",
    "    return [title, press, datetime, article, good, warm, sad, angry, want, recommend, news_url]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747301e7",
   "metadata": {},
   "source": [
    "### comments_scraping(): 뉴스 댓글 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3943947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_scraping(news_url, wd):\n",
    "    try:\n",
    "        btn_view = wd.find_element(By.CLASS_NAME, 'u_cbox_btn_view_comment').click() #댓글 더 보기\n",
    "        print(\"[댓글 더 보기]\", end=\"\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        while True: #댓글 더보기 이후 더 보기를 계속 클릭해서 끝까지 가야하므로 while문을 써준다.\n",
    "            wd.find_element(By.CLASS_NAME,'u_cbox_page_more').click() #더 보기\n",
    "            print(\"[더 보기]\", end=\"\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    btn_reply_list = wd.find_elements(By.CLASS_NAME, 'u_cbox_btn_reply') #답글 버튼 들고 옴\n",
    "    for btn_reply in btn_reply_list:\n",
    "        btn_reply.send_keys('\\n') #.click()으로 변화가 없는 부분은 이렇게 처리 한다.\n",
    "        print(\"[답글]\", end=\"\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    print(\"[댓글 스크래핑]\")\n",
    "    \n",
    "    comments_idx = 0\n",
    "    comments_df = pd.DataFrame(columns=(\"Contents\",\"Name\",\"Datetime\",\"Like\",\"Dislike\",\"URL\"))\n",
    "    \n",
    "    \n",
    "    comments = wd.find_elements(By.CLASS_NAME, 'u_cbox_comment_box') #댓글 박스 들고 옴\n",
    "    for comment in comments:\n",
    "        try:\n",
    "            name = comment.find_element(By.CLASS_NAME, 'u_cbox_nick').text\n",
    "            date = comment.find_element(By.CLASS_NAME, 'u_cbox_date').text\n",
    "            contents = comment.find_element(By.CLASS_NAME, 'u_cbox_contents').text\n",
    "            recomm = comment.find_element(By.CLASS_NAME, 'u_cbox_cnt_recomm').text\n",
    "            unrecomm = comment.find_element(By.CLASS_NAME, 'u_cbox_cnt_unrecomm').text\n",
    "            print(f\"  댓글 #{comments_idx+1}:\", [contents, name, date, recomm, unrecomm, news_url])\n",
    "            comments_df.loc[comments_idx] = [contents, name, date, recomm, unrecomm, news_url]\n",
    "            comments_idx += 1\n",
    "        except NoSuchElementException:\n",
    "#             print(\"[삭제되거나 부적절한 댓글]\")\n",
    "            continue\n",
    "    \n",
    "    return comments_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4212ee",
   "metadata": {},
   "source": [
    "### scrpaing() : 스크래핑 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317de291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(news_max = 2):\n",
    "    wd = webdriver.Chrome('chromedriver', options=chrome_options)\n",
    "    wd.implicitly_wait(3) #대기 시간\n",
    "    \n",
    "    # --- 검색을 위한 세팅 ---\n",
    "    wd.execute_script('window.open(\"about:blank\", \"_blank\");') #JS의 새 탭을 띄우는 용도\n",
    "    tabs = wd.window_handles #탭 핸들링이 가능해 짐\n",
    "    wd.switch_to.window(tabs[0]) #첫번째 탭으로 전환\n",
    "    query = input(\"검색어 입력: \")\n",
    "    search_url = \"https://search.naver.com/search.naver?where=news&ie=utf8&sm=nws_hty&query=\" + query\n",
    "    wd.get(search_url)\n",
    "    \n",
    "    \n",
    "    # --- Data-Frame ---\n",
    "    news_idx = 0\n",
    "    news_df = pd.DataFrame(columns=(\"Title\",\"Press\",\"DateTime\",\"Article\",\"Good\",\"Warm\",\"Sad\",\"Angry\",\"Want\",\"Recommend\",\"URL\"))\n",
    "    comments_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -- 검색 이후 '네이버 뉴스' 만 스크래핑 ---\n",
    "    while True:\n",
    "        info_list = wd.find_elements(By.CLASS_NAME, 'info_group')\n",
    "        for info in info_list:\n",
    "            try:\n",
    "                first_step = info.find_elements(By.TAG_NAME, 'a')\n",
    "                news_url = first_step[1].get_attribute('href') #'네이버 뉴스' 의 링크를 가져 옴\n",
    "                time.sleep(1)\n",
    "                print(news_url)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            wd.switch_to.window(tabs[1])\n",
    "            wd.get(news_url)\n",
    "\n",
    "            news_df.loc[news_idx] = news_scraping(news_url, wd)\n",
    "            news_idx += 1\n",
    "        \n",
    "            # --- 코멘트를 PD로 만들고 계속 합쳐서 만듬 ---\n",
    "            df = comments_scraping(news_url, wd)\n",
    "            comments_df = pd.concat([comments_df, df])\n",
    "            \n",
    "            if news_idx >= news_max:\n",
    "                break\n",
    "                \n",
    "        if news_idx >= news_max:\n",
    "            break\n",
    "                \n",
    "        \n",
    "        # --- 페이징 ---\n",
    "        try:\n",
    "            wd.switch_to.window(tabs[0]) #탭[1]에서 스크래핑이 다 끝나면 다시 텝[0]으로 돌아오고 다음 페이지로 이동 시킴\n",
    "            wd.find_element(By.CLASS_NAME, 'btn_next').click()\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    wd.close()\n",
    "    \n",
    "    return news_df, comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0fd6ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 인공지능\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=105&oid=030&aid=0002993312\n",
      "[댓글 스크래핑]\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=215&aid=0001008785\n",
      "[댓글 스크래핑]\n"
     ]
    }
   ],
   "source": [
    "news_df, comments_df = scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01040cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Press</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Article</th>\n",
       "      <th>Good</th>\n",
       "      <th>Warm</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Angry</th>\n",
       "      <th>Want</th>\n",
       "      <th>Recommend</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코웨이 노블 정수기 품질, 인공지능이 인증했다</td>\n",
       "      <td>전자신문</td>\n",
       "      <td>2022.01.12. 오후 2:02</td>\n",
       "      <td>코웨이는 프리미엄 디자인 노블 정수기가 국내 최초로 'AI+(에이아이플러스)' 인공...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>https://news.naver.com/main/read.naver?mode=LS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>얼굴인식 인공지능(AI) 스타트업 메사쿠어컴퍼니, 80억 시리즈A 투자유치</td>\n",
       "      <td>한국경제TV</td>\n",
       "      <td>2022.01.12. 오전 10:00</td>\n",
       "      <td>제1금융권 모바일뱅킹에 얼굴인식 기술 적용모바일운전면허증 구축사업 얼굴인식 솔루션 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>https://news.naver.com/main/read.naver?mode=LS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title   Press              DateTime  \\\n",
       "0                  코웨이 노블 정수기 품질, 인공지능이 인증했다    전자신문   2022.01.12. 오후 2:02   \n",
       "1  얼굴인식 인공지능(AI) 스타트업 메사쿠어컴퍼니, 80억 시리즈A 투자유치  한국경제TV  2022.01.12. 오전 10:00   \n",
       "\n",
       "                                             Article Good Warm Sad Angry Want  \\\n",
       "0  코웨이는 프리미엄 디자인 노블 정수기가 국내 최초로 'AI+(에이아이플러스)' 인공...    0    0   0     0    0   \n",
       "1  제1금융권 모바일뱅킹에 얼굴인식 기술 적용모바일운전면허증 구축사업 얼굴인식 솔루션 ...    0    0   0     0    0   \n",
       "\n",
       "  Recommend                                                URL  \n",
       "0            https://news.naver.com/main/read.naver?mode=LS...  \n",
       "1            https://news.naver.com/main/read.naver?mode=LS...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8bf3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contents</th>\n",
       "      <th>Name</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Like</th>\n",
       "      <th>Dislike</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Contents, Name, Datetime, Like, Dislike, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebf799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
